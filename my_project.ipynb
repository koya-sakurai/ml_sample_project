{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像が保存されているディレクトリのパス\n",
    "image_root_dir = \"./image/files/\"\n",
    "# 分類カテゴリ（画像が保存されているフォルダ名とイコールになる）\n",
    "categories = [\"醤油ラーメン\",\"味噌ラーメン\",\"担々麺\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ラベリングによる学習/検証データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from functools import partial\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import random, math\n",
    "\n",
    "# monkey patch\n",
    "np.load = partial(np.load, allow_pickle=True)\n",
    "\n",
    "# 画像データ用配列\n",
    "X = []\n",
    "# ラベルデータ用配列\n",
    "Y = []\n",
    "\n",
    "# 画像データごとにadd_sample()を呼び出し、X,Yの配列を返す関数\n",
    "def make_sample(files):\n",
    "    global X, Y\n",
    "    X = []\n",
    "    Y = []\n",
    "    for cat, fname in files:\n",
    "        add_sample(cat, fname)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# 渡された画像データを読み込んでXに格納し、また、\n",
    "# 画像データに対応するcategoriesのidxをY格納する関数\n",
    "def add_sample(cat, fname):\n",
    "    img = Image.open(fname)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((256, 256))\n",
    "    data = np.asarray(img)\n",
    "    X.append(data)\n",
    "    Y.append(cat)\n",
    "\n",
    "# 全データ格納用配列\n",
    "allfiles = []\n",
    "\n",
    "# カテゴリ配列の各値と、それに対応するidxを認識し、全データをallfilesにまとめる\n",
    "for idx, cat in enumerate(categories):\n",
    "    image_dir = image_root_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir + \"/*.jpg\")\n",
    "    for f in files:\n",
    "        allfiles.append((idx, f))\n",
    "\n",
    "# シャッフル後、学習データと検証データに分ける\n",
    "random.shuffle(allfiles)\n",
    "th = math.floor(len(allfiles) * 0.8)\n",
    "train = allfiles[0:th]\n",
    "test  = allfiles[th:]\n",
    "X_train, y_train = make_sample(train)\n",
    "X_test, y_test = make_sample(test)\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "# データを保存する\n",
    "np.save(\"./data/my_data.npy\", xy)\n",
    "print(\"saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(256, 256, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation=\"relu\"))\n",
    "model.add(layers.Dense(len(categories), activation=\"sigmoid\")) # 分類先の種類分設定\n",
    "\n",
    "# モデル構成の確認\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルのコンパイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "nb_classes = len(categories)\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load(\"./data/my_data.npy\")\n",
    "\n",
    "# データの正規化\n",
    "X_train = X_train.astype(\"float\") / 255\n",
    "X_test  = X_test.astype(\"float\")  / 255\n",
    "\n",
    "# kerasで扱えるようにcategoriesをベクトルに変換\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test  = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=8, # 何回学習を繰り返すか\n",
    "                  batch_size=6, # データを何個ずつのサブセットに分けて学習するか\n",
    "                  validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習結果を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = model.history['acc']\n",
    "val_acc = model.history['val_acc']\n",
    "loss = model.history['loss']\n",
    "val_loss = model.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('./data/accuracy.png')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.savefig('./data/loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル・重みの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル\n",
    "json_string = model.model.to_json()\n",
    "open('./data/my_predict.json', 'w').write(json_string)\n",
    "\n",
    "# 重み\n",
    "hdf5_file = \"./data/my_predict.hdf5\"\n",
    "model.model.save_weights(hdf5_file)\n",
    "print(\"saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
