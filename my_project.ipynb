{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ラベリングによる学習/検証データの準備\n",
    "\n",
    "from PIL import Image\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import random, math\n",
    "\n",
    "#画像が保存されているルートディレクトリのパス\n",
    "root_dir = \"./images\"\n",
    "# 商品名\n",
    "categories = [\"醤油ラーメン\",\"味噌ラーメン\",\"塩ラーメン\",\"豚骨ラーメン\",\"担々麺\"]\n",
    "\n",
    "# 画像データ用配列\n",
    "X = []\n",
    "# ラベルデータ用配列\n",
    "Y = []\n",
    "\n",
    "#画像データごとにadd_sample()を呼び出し、X,Yの配列を返す関数\n",
    "def make_sample(files):\n",
    "    global X, Y\n",
    "    X = []\n",
    "    Y = []\n",
    "    for cat, fname in files:\n",
    "        add_sample(cat, fname)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "#渡された画像データを読み込んでXに格納し、また、\n",
    "#画像データに対応するcategoriesのidxをY格納する関数\n",
    "def add_sample(cat, fname):\n",
    "    img = Image.open(fname)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((256, 256))\n",
    "    data = np.asarray(img)\n",
    "    X.append(data)\n",
    "    Y.append(cat)\n",
    "\n",
    "#全データ格納用配列\n",
    "allfiles = []\n",
    "\n",
    "#カテゴリ配列の各値と、それに対応するidxを認識し、全データをallfilesにまとめる\n",
    "for idx, cat in enumerate(categories):\n",
    "    image_dir = root_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir + \"/*.jpg\")\n",
    "    for f in files:\n",
    "        allfiles.append((idx, f))\n",
    "\n",
    "#シャッフル後、学習データと検証データに分ける\n",
    "random.shuffle(allfiles)\n",
    "th = math.floor(len(allfiles) * 0.8)\n",
    "train = allfiles[0:th]\n",
    "test  = allfiles[th:]\n",
    "X_train, y_train = make_sample(train)\n",
    "X_test, y_test = make_sample(test)\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "#データを保存する（データの名前を「tea_data.npy」としている）\n",
    "np.save(\"./data/ramen_data.npy\", xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータを作成\n",
    "from PIL import Image\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import random, math\n",
    "\n",
    "# 画像が保存されているディレクトリのパス\n",
    "root_dir = \"./images\"\n",
    "# 画像が保存されているフォルダ名\n",
    "categories = [\"醤油ラーメン\",\"味噌ラーメン\",\"塩ラーメン\",\"豚骨ラーメン\",\"担々麺\"]\n",
    "\n",
    "X = [] # 画像データ\n",
    "Y = [] # ラベルデータ\n",
    "\n",
    "# フォルダごとに分けられたファイルを収集\n",
    "#（categoriesのidxと、画像のファイルパスが紐づいたリストを生成）\n",
    "allfiles = []\n",
    "for idx, cat in enumerate(categories):\n",
    "    image_dir = root_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir + \"/*.jpg\")\n",
    "    for f in files:\n",
    "        allfiles.append((idx, f))\n",
    "\n",
    "for cat, fname in allfiles:\n",
    "    img = Image.open(fname)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((256, 256))\n",
    "    data = np.asarray(img)\n",
    "    X.append(data)\n",
    "    Y.append(cat)\n",
    "\n",
    "x = np.array(X)\n",
    "y = np.array(Y)\n",
    "\n",
    "np.save(\"./data/ramen_data_test_X_150.npy\", x)\n",
    "np.save(\"./data/ramen_data_test_Y_150.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 15:44:41.693379 4373079488 deprecation_wrapper.py:119] From /Users/sakurai/Documents/learn-tensorflow/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0819 15:44:41.703962 4373079488 deprecation_wrapper.py:119] From /Users/sakurai/Documents/learn-tensorflow/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0819 15:44:41.705894 4373079488 deprecation_wrapper.py:119] From /Users/sakurai/Documents/learn-tensorflow/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0819 15:44:41.716428 4373079488 deprecation_wrapper.py:119] From /Users/sakurai/Documents/learn-tensorflow/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 254, 254, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 125, 125, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 60, 60, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 13,088,965\n",
      "Trainable params: 13,088,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#モデルの構築\n",
    "\n",
    "from keras import layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3),activation=\"relu\",input_shape=(256,256,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3),activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3),activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3),activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512,activation=\"relu\"))\n",
    "model.add(layers.Dense(5,activation=\"sigmoid\")) #分類先の種類分設定\n",
    "\n",
    "#モデル構成の確認\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 15:44:41.787335 4373079488 deprecation_wrapper.py:119] From /Users/sakurai/Documents/learn-tensorflow/tensorflow/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0819 15:44:41.792314 4373079488 deprecation_wrapper.py:119] From /Users/sakurai/Documents/learn-tensorflow/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0819 15:44:41.795454 4373079488 deprecation.py:323] From /Users/sakurai/Documents/learn-tensorflow/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#モデルのコンパイル\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの準備\n",
    "\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "categories = [\"醤油ラーメン\",\"味噌ラーメン\",\"塩ラーメン\",\"豚骨ラーメン\",\"担々麺\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load(\"./data/ramen_data.npy\")\n",
    "\n",
    "#データの正規化\n",
    "X_train = X_train.astype(\"float\") / 255\n",
    "X_test  = X_test.astype(\"float\")  / 255\n",
    "\n",
    "#kerasで扱えるようにcategoriesをベクトルに変換\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test  = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 15:44:42.605498 4373079488 deprecation_wrapper.py:119] From /Users/sakurai/Documents/learn-tensorflow/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 381 samples, validate on 96 samples\n",
      "Epoch 1/10\n",
      "381/381 [==============================] - 20s 52ms/step - loss: 0.4892 - acc: 0.7974 - val_loss: 0.5147 - val_acc: 0.7604\n",
      "Epoch 2/10\n",
      "381/381 [==============================] - 20s 51ms/step - loss: 0.4222 - acc: 0.8157 - val_loss: 0.4329 - val_acc: 0.8125\n",
      "Epoch 3/10\n",
      "381/381 [==============================] - 19s 50ms/step - loss: 0.3783 - acc: 0.8325 - val_loss: 0.4419 - val_acc: 0.8083\n",
      "Epoch 4/10\n",
      "381/381 [==============================] - 20s 54ms/step - loss: 0.3479 - acc: 0.8483 - val_loss: 0.5334 - val_acc: 0.7625\n",
      "Epoch 5/10\n",
      "381/381 [==============================] - 19s 50ms/step - loss: 0.3073 - acc: 0.8772 - val_loss: 0.4186 - val_acc: 0.8396\n",
      "Epoch 6/10\n",
      "381/381 [==============================] - 19s 50ms/step - loss: 0.2786 - acc: 0.8882 - val_loss: 0.4291 - val_acc: 0.8187\n",
      "Epoch 7/10\n",
      "381/381 [==============================] - 19s 51ms/step - loss: 0.2389 - acc: 0.9097 - val_loss: 0.4996 - val_acc: 0.7875\n",
      "Epoch 8/10\n",
      "381/381 [==============================] - 19s 51ms/step - loss: 0.2114 - acc: 0.9197 - val_loss: 0.4480 - val_acc: 0.8313\n",
      "Epoch 9/10\n",
      "381/381 [==============================] - 20s 52ms/step - loss: 0.1763 - acc: 0.9354 - val_loss: 0.5375 - val_acc: 0.8083\n",
      "Epoch 10/10\n",
      "381/381 [==============================] - 20s 52ms/step - loss: 0.1414 - acc: 0.9512 - val_loss: 0.5692 - val_acc: 0.8083\n"
     ]
    }
   ],
   "source": [
    "#モデルの学習\n",
    "\n",
    "model = model.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=10,\n",
    "                  batch_size=6,\n",
    "                  validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習結果を表示\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = model.history['acc']\n",
    "val_acc = model.history['val_acc']\n",
    "loss = model.history['loss']\n",
    "val_loss = model.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('./data/accuracy.png')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.savefig('./data/loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルの保存\n",
    "\n",
    "json_string = model.model.to_json()\n",
    "open('./data/ramen_predict.json', 'w').write(json_string)\n",
    "\n",
    "#重みの保存\n",
    "\n",
    "hdf5_file = \"./data/ramen_predict.hdf5\"\n",
    "model.model.save_weights(hdf5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import random, math\n",
    "\n",
    "# 画像が保存されているディレクトリのパス\n",
    "root_dir = \"./images\"\n",
    "# 画像が保存されているフォルダ名\n",
    "categories = [\"醤油ラーメン\",\"味噌ラーメン\",\"塩ラーメン\",\"豚骨ラーメン\",\"担々麺\"]\n",
    "\n",
    "X = [] # 画像データ\n",
    "Y = [] # ラベルデータ\n",
    "\n",
    "# フォルダごとに分けられたファイルを収集\n",
    "#（categoriesのidxと、画像のファイルパスが紐づいたリストを生成）\n",
    "allfiles = []\n",
    "for idx, cat in enumerate(categories):\n",
    "    image_dir = root_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir + \"/*.jpg\")\n",
    "    for f in files:\n",
    "        allfiles.append((idx, f))\n",
    "\n",
    "for cat, fname in allfiles:\n",
    "    img = Image.open(fname)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((256, 256))\n",
    "    data = np.asarray(img)\n",
    "    X.append(data)\n",
    "    Y.append(cat)\n",
    "\n",
    "x = np.array(X)\n",
    "y = np.array(Y)\n",
    "\n",
    "np.save(\"./data/ramen_data_test_X_256.npy\", x)\n",
    "np.save(\"./data/ramen_data_test_Y_256.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477/477 [==============================] - 4s 8ms/step\n",
      "loss= 1.7751038129474632\n",
      "accuracy= 0.8876310198312035\n"
     ]
    }
   ],
   "source": [
    "# モデルの精度を測る\n",
    "\n",
    "#評価用のデータの読み込み\n",
    "test_X = np.load(\"./data/ramen_data_test_X_256.npy\")\n",
    "test_Y = np.load(\"./data/ramen_data_test_Y_256.npy\")\n",
    "\n",
    "#Yのデータをone-hotに変換\n",
    "from keras.utils import np_utils\n",
    "\n",
    "test_Y = np_utils.to_categorical(test_Y, 5) # 分類先の種類分設定\n",
    "\n",
    "score = model.model.evaluate(x=test_X,y=test_Y)\n",
    "\n",
    "print('loss=', score[0])\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
